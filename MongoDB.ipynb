{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoSQL solution: MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://img.icons8.com/color/48/000000/mongodb.png \"MongoDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation on local machine (MacOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install brew package manager: [brew website](https://brew.sh/#install)\n",
    "* From terminal issue the following commands:\n",
    "```shell\n",
    "brew tap mongodb/brew\n",
    "```\n",
    "* Install the latest available release of MongoDB coommunity server\n",
    "```shell\n",
    "brew install mongodb-community\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Unstructured dataset: Archive.org Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to dataset](https://archive.org/details/archiveteam-twitter-stream-2019-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets were scraped from the \"Spritzer\" Twitter stream, which is a realtime stream of a random sample of `1%` of all tweets. There are approximately `500 million` tweets for each day.\n",
    "The dataset considered refers to the month of June.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decompressed dataset contains `151 million` tweets and occupies `449.5 GiB`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tackled the relatively big dimension of the dataset by automating the exctraction of the compressed folders and the insertion into a MongoDB storing data in an external hard drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to MongoDB running on local machine with pymongo module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.son import SON\n",
    "from bson.regex import Regex\n",
    "\n",
    "# Connecting to MongoDB\n",
    "client = MongoClient()\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "# Creation of DB and collection\n",
    "db = client.finalmongotweets\n",
    "collection = db.collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompressig Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "tweets = \"/Volumes/G-HDD/Tweets/\"\n",
    "\n",
    "dirs = [f.path for f in os.scandir(tweets) if f.is_dir()]\n",
    "#29\n",
    "for loop1 in range(19,29): # range(len(dirs)):\n",
    "    file_counter = 0\n",
    "    subdirs = [f.path for f in os.scandir(dirs[loop1]) if f.is_dir()]\n",
    "    for loop2 in range(len(subdirs)):\n",
    "        print(f\"Currently going through {subdirs[loop2]}\")\n",
    "        files = [f for f in os.listdir(subdirs[loop2]) if os.path.isfile(os.path.join(subdirs[loop2], f))]\n",
    "        for i in files:\n",
    "            if i.endswith('.json.bz2'):\n",
    "                file_counter += 1\n",
    "                filepath = os.path.join(subdirs[loop2], i)\n",
    "                newfilepath = os.path.join(subdirs[loop2], \"{0}.json\".format(file_counter))\n",
    "                with open(newfilepath, 'wb') as new_file, bz2.BZ2File(filepath, 'rb') as file:\n",
    "                    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "                        new_file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = \"/Volumes/G-HDD/Tweets/\"\n",
    "\n",
    "dirs = [f.path for f in os.scandir(tweets) if f.is_dir()]\n",
    "\n",
    "for loop1 in range(3): # range(len(dirs)):\n",
    "    subdirs = [f.path for f in os.scandir(dirs[loop1]) if f.is_dir()]\n",
    "    for loop2 in range(3): # range(len(subdirs)):\n",
    "        print(f\"Currently going through {subdirs[loop2]}\")\n",
    "        files = [f for f in os.listdir(subdirs[loop2]) if os.path.isfile(os.path.join(subdirs[loop2], f))]\n",
    "        for i in files:\n",
    "            if i.endswith('.json'):\n",
    "                print(f\"Currently adding {i}\")\n",
    "                data = [json.loads(line) for line in open(os.path.join(subdirs[loop2], i), 'r')]\n",
    "                db.collection.insert_many(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Structured dataset: Scraping Twitter users' timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns .csv files containing the latest 3200 tweets (Twitter API limitation) from each account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accounts considered following [this article](https://www.forbes.com/sites/alapshah/2017/11/16/the-100-best-twitter-accounts-for-finance/):\n",
    "```\n",
    "['John_Hempton', 'BarbarianCap', 'muddywatersre', 'AlderLaneeggs', 'CitronResearch', 'BrattleStCap', 'KerrisdaleCap', 'modestproposal1', 'marketfolly', 'EventDrivenMgr', 'ActivistShorts', 'Carl_C_Icahn', 'LongShortTrader', 'DonutShorts', 'sprucepointcap', 'BluegrassCap', 'SIRF_Report', 'NoonSixCap', 'WallStCynic', 'GothamResearch', 'herbgreenberg', 'Valuetrap13', 'valuewalk', 'UnionSquareGrp', 'PlanMaestro', 'ReformedBroker', 'SkeleCap', 'FatTailCapital', 'ShortSightedCap', 'footnoted', 'Mega_Man_2', 'JacobWolinsky', 'zerohedge', 'FundyLongShort', 'MugatuCapital', 'DumbLuckCapital', 'Hedge_FundGirl', 'PresciencePoint', 'DavidSchawel', 'pmarca', 'fundiescapital', 'ActAccordingly', 'EquityNYC', 'nosunkcosts', 'MicroFundy', 'BergenCapital', 'marginalidea', 'Keubiko', 'Jesse_Livermore', 'PainCapital']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication, after creating a Twitter developer account\n",
    "\n",
    "import csv\n",
    "import tweepy as tw\n",
    "\n",
    "consumer_key = 'mykey'\n",
    "consumer_secret = 'mysecret'\n",
    "access_token = 'mytoken'\n",
    "access_token_secret = 'mytokensecret'\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth)\n",
    "\n",
    "# List of accounts that are considered experts in the financial\n",
    "accounts = ['marketfolly', 'EventDrivenMgr', 'ActivistShorts', 'Carl_C_Icahn', 'LongShortTrader', 'DonutShorts', 'sprucepointcap', 'BluegrassCap', 'SIRF_Report', 'NoonSixCap', 'WallStCynic', 'GothamResearch', 'herbgreenberg', 'Valuetrap13', 'valuewalk', 'UnionSquareGrp', 'PlanMaestro', 'ReformedBroker', 'SkeleCap', 'FatTailCapital', 'ShortSightedCap', 'footnoted', 'Mega_Man_2', 'JacobWolinsky', 'zerohedge', 'FundyLongShort', 'MugatuCapital', 'DumbLuckCapital', 'Hedge_FundGirl', 'PresciencePoint', 'DavidSchawel', 'pmarca', 'fundiescapital', 'ActAccordingly', 'EquityNYC', 'nosunkcosts', 'MicroFundy', 'BergenCapital', 'marginalidea', 'Keubiko', 'Jesse_Livermore', 'PainCapital']\n",
    "\n",
    "# Scraping the accounts\n",
    "for account in accounts:\n",
    "    twdata = []\n",
    "    print(f\"Currently scraping {account} twitter account.\")\n",
    "\n",
    "    for tweet in tw.Cursor(api.user_timeline, id=account, tweet_mode=\"extended\").items(3200):\n",
    "        user = tweet.user.screen_name\n",
    "        tweets = tweet.full_text\n",
    "        tweet_id = tweet.id\n",
    "        date = tweet.created_at\n",
    "        rts = tweet.retweet_count\n",
    "        fav = tweet.favorite_count\n",
    "        hashtags = ','.join([hashtag[\"text\"] for hashtag in tweet.entities[\"hashtags\"]])\n",
    "        source = tweet.source\n",
    "        twdata.append([user, tweets, tweet_id, date, rts, fav, hashtags, source])\n",
    "        \n",
    "    # Saving to csv\n",
    "    csv_file = account + \"_tweets.csv\"\n",
    "    print(f\">>> Saving {csv_file}.\")\n",
    "\n",
    "    with open(csv_file, 'w+') as csvFile:\n",
    "        writer = csv.writer(csvFile, delimiter=',')\n",
    "        writer.writerow(['Username', 'Tweets', 'Tweet ID', 'Created At', 'Retweets', 'Favorites', 'Hashtags', 'Source'])\n",
    "        writer.writerows(twdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Financial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historical financial data were retrieved from [Yahoo Finance](https://finance.yahoo.com/). In order to retrieve more data points, like intraday prices, World Trading Data API can be used, however the number of free requests were very limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requesting single stock historical data from WorldTradingData API\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'https://api.worldtradingdata.com/api/v1/history' # for intraday: https://intraday.worldtradingdata.com/api/v1/intraday\n",
    "params = {\n",
    "  'symbol': 'TSLA',\n",
    "  'api_token': 'mykey'\n",
    "  'date_from': '2019-01-01'\n",
    "  'date_to': '2020-01-01'\n",
    "}\n",
    "response = requests.request('GET', url, params=params)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Returns all the tweets in english that contain `tsla` or `tesla` in their text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use tweets;\n",
    "db.myTry.find(\n",
    "    { \n",
    "        \"lang\" : \"en\", \n",
    "        \"text\" : /.tesla./i, /.tsla./i\n",
    "    });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Counts the number of tweets posted per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use tweets;\n",
    "db.getCollection(\"myTry\").aggregate(\n",
    "    [ { \n",
    "            \"$group\" : { \n",
    "                \"_id\" : { \n",
    "                    \"lang\" : \"$lang\"\n",
    "                }, \n",
    "                \"COUNT(*)\" : { \n",
    "                    \"$sum\" : NumberInt(1)\n",
    "                }\n",
    "            }\n",
    "        }, \n",
    "        { \n",
    "            \"$project\" : { \n",
    "                \"lang\" : \"$_id.lang\", \n",
    "                \"COUNT(*)\" : \"$COUNT(*)\", \n",
    "                \"_id\" : NumberInt(0)\n",
    "            }\n",
    "        }\n",
    "    ]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Counts the total number of tweets per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use tweets;\n",
    "db.getCollection(\"myTry\").aggregate(\n",
    "    [\n",
    "        { \n",
    "            \"$group\" : { \n",
    "                \"_id\" : { \n",
    "                    \"id\" : \"$id\"\n",
    "                }, \n",
    "                \"COUNT(*)\" : { \n",
    "                    \"$sum\" : NumberInt(1)\n",
    "                }\n",
    "            }\n",
    "        }, \n",
    "        { \n",
    "            \"$project\" : { \n",
    "                \"id\" : \"$_id.id\", \n",
    "                \"COUNT(*)\" : \"$COUNT(*)\", \n",
    "                \"_id\" : NumberInt(0)\n",
    "            }\n",
    "        }\n",
    "    ]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Counts the number of tweets per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add a new field to convert 'created_at' field from string to date datatype; save the new information with the whole collection into a view (convertDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use tweets;\n",
    "\n",
    "dateConversionStage = {\n",
    "   $addFields: {\n",
    "      convertedDate: { $toDate: \"$created_at\" }\n",
    "   }\n",
    "};\n",
    "\n",
    "#using view support from studio 3T\n",
    "db.myTry.aggregate([\n",
    "    dateConversionStage\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From previous view extract the most important data and split the converted date into date and time. This data are the ones which are going to be used in our analysis. Saved fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create view using support from studio 3T\n",
    "db.getCollection(\"convertDate\").aggregate([   \n",
    "    { \n",
    "        \"$match\" : { \n",
    "            \"lang\" : \"en\",\n",
    "            \"text\" : /^.* tesla .*$/i\n",
    "        }\n",
    "    },\n",
    "    { \n",
    "        \"$project\" : { \n",
    "            \"user\" : \"$user.screen_name\", \n",
    "            \"source\" : \"$source\", \n",
    "            \"geo\" : \"$geo\", \n",
    "            \"coordinates\" : \"coordinates\", \n",
    "            \"place\" : \"$place\", \n",
    "            \"text\" : \"$text\", \n",
    "            \"lang\" : \"$lang\", \n",
    "            \"date\" : { \n",
    "                \"$dateToString\" : { \n",
    "                    \"format\" : \"%Y-%m-%d\", \n",
    "                    \"date\" : \"$convertedDate\"\n",
    "                }\n",
    "            }, \n",
    "            \"time\" : { \n",
    "                \"$dateToString\" : { \n",
    "                    \"format\" : \"%H:%M:%S:%L\", \n",
    "                    \"date\" : \"$convertedDate\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "], \n",
    "{ \n",
    "    \"allowDiskUse\" : false\n",
    "}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From ImpData group and count the number of tweets posted per each day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use tweets;\n",
    "db.getCollection(\"ImpData\").aggregate(\n",
    "    [   { \n",
    "            \"$group\" : { \n",
    "                \"_id\" : { \n",
    "                    \"date\" : \"$date\"\n",
    "                }, \n",
    "                \"COUNT(*)\" : { \n",
    "                    \"$sum\" : NumberInt(1)\n",
    "                }\n",
    "            }\n",
    "        }, \n",
    "        { \n",
    "            \"$project\" : { \n",
    "                \"date\" : \"$_id.date\", \n",
    "                \"COUNT(*)\" : \"$COUNT(*)\", \n",
    "                \"_id\" : NumberInt(0)\n",
    "            }\n",
    "        }\n",
    "    ], \n",
    "    { \n",
    "        \"allowDiskUse\" : true\n",
    "    }\n",
    ");\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
